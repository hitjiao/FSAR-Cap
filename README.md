# FSAR-Cap
 a large-scale SAR captioning dataset with 14,480 images and 72,400 image–text pairs.
Synthetic Aperture Radar (SAR) image captioning enables scene-level semantic understanding and plays a crucial role in applications such as military intelligence and urban planning, but its development is limited by the scarcity of high-quality datasets. To address this, we present FSAR-Cap, a large-scale SAR captioning dataset with 14,480 images and 72,400 image–text pairs. FSAR-Cap is built on the FAIR-CSAR detection dataset and constructed through a two-stage annotation strategy that combines hierarchical template-based representation, manual verification and supplementation, prompt standardization. Compared with existing resources, FSAR-Cap provides richer fine-grained annotations, broader category coverage, and higher annotation quality. Benchmarking with multiple encoder–decoder architectures verifies its effectiveness, establishing a foundation for future research in SAR captioning and intelligent image interpretation. 

In future work, FSAR-Cap can serve as a valuable benchmark for the community. We anticipate the emergence of captioning models specifically tailored for SAR imagery, and the dataset may further facilitate the development of more robust and generalizable VLMs in the SAR domain.

You can visit this link to download and use our dataset:https://www.scidb.cn/en/detail?dataSetId=e4c17cc137e74878b46c03d8650ffa78&version=V1&code=o00118


The analysis of the labeled data is as follows, (a) Frequency histogram of the captions length per image. (b) Word cloud image of the primary annotation words in the dataset. (c) Radar plot of the main data in the FSAR-Cap dataset.
![radar1](https://github.com/user-attachments/assets/11ecdb06-f505-4d75-a18e-f60682167b51)

